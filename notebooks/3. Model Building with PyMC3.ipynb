{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models in PyMC3\n",
    "\n",
    "Bayesian inference begins with specification of a probability model relating unknown variables to data. PyMC3 provides the basic building blocks for Bayesian probability models: stochastic random variables, deterministic variables, and factor potentials. \n",
    "\n",
    "A **stochastic random variable** is a factor whose value is not completely determined by its parents, while the value of a **deterministic random variable** is entirely determined by its parents. Most models can be constructed using only these two variable types. The third quantity, the **factor potential**, is *not* a variable but simply a\n",
    "log-likelihood term or constraint that is added to the joint log-probability to modify it. \n",
    "\n",
    "## The FreeRV class\n",
    "\n",
    "A stochastic variable is represented in PyMC3 by a `FreeRV` class. This structure adds functionality to Theano's `TensorVariable` class, by mixing in the PyMC `Factor` class. A `Factor` is used whenever a variable contributes a log-probability term to a model. Hence, you know a variable is a subclass of `Factor` whenever it has a `logp` method, as we saw in the previous section.\n",
    "\n",
    "A `FreeRV` object has several important attributes:\n",
    "\n",
    "`dshape`\n",
    ":   The variable's shape.\n",
    "\n",
    "`dsize`\n",
    ":   The overall size of the variable.\n",
    "\n",
    "`distribution`\n",
    ":   The probability density or mass function that describes the distribution of the variable's values.\n",
    "\n",
    "`logp`\n",
    ":   The log-probability of the variable's current value given the values\n",
    "    of its parents.\n",
    "\n",
    "`init_value`\n",
    ":   The initial value of the variable, used by many algorithms as a starting point for model fitting.\n",
    "\n",
    "`model`\n",
    ":   The PyMC model to which the variable belongs.\n",
    "\n",
    "\n",
    "### Creation of stochastic random variables\n",
    "\n",
    "There are two ways to create stochastic random variables (`FreeRV` objects), which we will call the **automatic**, and **manual** interfaces.\n",
    "\n",
    "#### Automatic\n",
    "\n",
    "Stochastic random variables with standard distributions provided by PyMC3 can be created in a single line using special subclasses of the `Distribution` class. For example, as we have seen, the uniformly-distributed discrete variable $switchpoint$ in the coal mining disasters model is created using the automatic interface as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "with pm.Model() as disaster_model:\n",
    "\n",
    "    switchpoint = pm.DiscreteUniform('switchpoint', lower=0, upper=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the rate parameters can automatically be given exponential priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied log-transform to early_mean and added transformed early_mean_log to model.\n",
      "Applied log-transform to late_mean and added transformed late_mean_log to model.\n"
     ]
    }
   ],
   "source": [
    "with disaster_model:\n",
    "    early_mean = pm.Exponential('early_mean', lam=1)\n",
    "    late_mean = pm.Exponential('late_mean', lam=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC includes most of the probability density functions (for continuous variables) and probability mass functions (for discrete variables) used in probabilistic modeling. Continuous variables are represented by a specialized subclass of `Distribution` called `Continuous` and discrete variables by the `Discrete` subclass.\n",
    "\n",
    "The main difference between these two sublcasses are in the `dtype` attribute (`int64` for `Discrete` and `float64` for `Continuous`) and the `defaults` attribute, which determines which summary statistic to use for initial values when one is not specified ('mode' for `Discrete` and 'median', 'mean', and 'mode' for `Continuous`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mode']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switchpoint.distribution.defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we previewed in the introduction, `Distribution` has a class method `dist` that returns a probability distribution of that type, without being wrapped in a PyMC random variable object. Sometimes we wish to use a particular statistical distribution, without using it as a variable in a model; for example, to generate random numbers from the distribution. This class method allows that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymc3.distributions.continuous.Exponential at 0x1193be320>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm.Exponential.dist(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual\n",
    "\n",
    "The uniformly-distributed discrete stochastic variable `switchpoint` in the disasters model could alternatively be created from a function that computes its log-probability as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with pm.Model():\n",
    "    \n",
    "    def uniform_logp(value, lower=0, upper=111):\n",
    "        \"\"\"The switchpoint for the rate of disaster occurrence.\"\"\"\n",
    "        return pm.switch((value > upper) | (value < lower), -np.inf, -np.log(upper - lower + 1))\n",
    "\n",
    "    switchpoint = pm.DensityDist('switchpoint', logp=uniform_logp, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-4.718498871295094)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switchpoint.logp({'switchpoint':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-4.718498871295094)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switchpoint.logp({'switchpoint': 44})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-inf)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switchpoint.logp({'switchpoint':-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to notice: while the function specified for the `logp` argument can be an arbitrary Python function, it must use Theano operators in its body. This is because one or more of the arguments passed to the function may be `TensorVariables`, and they must be supported. Also, we passed the value to be evaluated by the `logp` function as a dictionary, rather than as a plain integer. By convention, values in PyMC3 are passed around as a data structure called a `Point`. Points in parameter space are represented by dictionaries with parameter names as they keys and the value of the parameters as the values.\n",
    "\n",
    "To emphasize, the Python function passed to `DensityDist` should compute the *log*-density or *log*-probability of the variable. That is why the return value in the example above is `-log(upper-lower+1)` rather than `1/(upper-lower+1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ObservedRV Class\n",
    "\n",
    "Stochastic random variables whose values are observed (*i.e.* data likelihoods) are represented by a different class than unobserved random variables. A `ObservedRV` object is instantiated any time a stochastic variable is specified with data passed as the `observed` argument. \n",
    "\n",
    "Otherwise, observed stochastic random variables are created via the same interfaces as unobserved: **automatic** or **manual**. As an example of an automatic instantiation, consider a Poisson data likelihood :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with disaster_model:\n",
    "    \n",
    "    disasters = pm.Poisson('disasters', mu=3, observed=[3,4,1,2,0,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen manual instantiation, from the melanoma survial model where the exponential survival likelihood was implemented manually:\n",
    "\n",
    "```python\n",
    "def logp(failure, value):\n",
    "    return (failure * log(lam) - lam * value).sum()\n",
    "\n",
    "x = DensityDist('x', logp, observed={'failure':failure, 'value':t})\n",
    "```\n",
    "\n",
    "Notice in this example that there are two vetors observed data for the likelihood `x`, passed as a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important responsibility of `ObservedRV` is to automatically handle missing values in the data, when they are present (absent?). More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Variables\n",
    "\n",
    "A deterministic variable is one whose values are *completely determined* by the values of their parents. For example, in our disasters model, `rate` is a deterministic variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with disaster_model:\n",
    "    \n",
    "    rate = pm.Deterministic('rate', pm.switch(switchpoint >= np.arange(112), early_mean, late_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so `rate`'s value can be computed exactly from the values of its parents `early_mean`, `late_mean` and `switchpoint`.\n",
    "\n",
    "There are two types of deterministic variables in PyMC3\n",
    "\n",
    "#### Anonymous deterministic variables\n",
    "\n",
    "The easiest way to create a deterministic variable is to operate on or transform one or more variables in a model directly. For example, the simplest way to specify the `rate` variable above is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with disaster_model:\n",
    "    \n",
    "    rate = pm.switch(switchpoint >= np.arange(112), early_mean, late_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, let's say we wanted to use the mean of the `early_mean` and `late_mean` variables somehere in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with disaster_model:\n",
    "    \n",
    "    mean_of_means = (early_mean + late_mean)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are called *anonymous* variables because we did not wrap it with a call to `Determinstic`, which gives it a name as its first argument. We simply specified the variable as a Python (or, Theano) expression. This is therefore the simplest way to construct a determinstic variable. The only caveat is that the values generated by anonymous determinstics at every iteration of a MCMC algorithm, for example, are not recorded to the resulting trace. So, this approach is only appropriate for intermediate values in your model that you do not wish to obtain posterior estimates for, alongside the other variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named deterministic variables\n",
    "\n",
    "To ensure that deterministic variables' values are accumulated during sampling, they should be instantiated using the **named deterministic** interface; this uses the `Deterministic` function to create the variable. Two things happen when a variable is created this way:\n",
    "\n",
    "1. The variable is given a name (passed as the first argument)\n",
    "2. The variable is appended to the model's list of random variables, which ensures that its values are tallied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rate_eval(switchpoint=switchpoint, early_mean=early_mean, late_mean=late_mean):\n",
    "    value = np.zeros(111)\n",
    "    value[:switchpoint] = early_mean\n",
    "    value[switchpoint:] = late_mean\n",
    "    return value\n",
    "\n",
    "rate = pm.Deterministic(eval = rate_eval,\n",
    "                  name = 'rate',\n",
    "                  parents = {'switchpoint': switchpoint, \n",
    "                          'early_mean': early_mean, \n",
    "                          'late_mean': late_mean},\n",
    "                  doc = 'The rate of disaster occurrence.',\n",
    "                  trace = True,\n",
    "                  verbose = 0,\n",
    "                  dtype=float,\n",
    "                  plot=False,\n",
    "                  cache_depth = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Potentials\n",
    "\n",
    "For some applications, we want to be able to modify the joint density by incorporating terms that don't correspond to probabilities of variables conditional on parents, for example:\n",
    "\n",
    "$$p(x_0, x_2, \\ldots x_{N-1}) \\propto \\prod_{i=0}^{N-2} \\psi_i(x_i, x_{i+1})$$\n",
    "\n",
    "In other cases we may want to add probability terms to existing models. For example, suppose we want to constrain the difference between the early and late means in the disaster model to be less than 1, so that the joint density becomes: \n",
    "\n",
    "$$p(y,\\tau,\\lambda_1,\\lambda_2) \\propto p(y|\\tau,\\lambda_1,\\lambda_2) p(\\tau) p(\\lambda_1) p(\\lambda_2) I(|\\lambda_2-\\lambda_1| \\lt 1)$$\n",
    "\n",
    "We call such log-probability terms **factor potentials** (Jordan 2004). Bayesian\n",
    "hierarchical notation doesn't accomodate these potentials. \n",
    "\n",
    "### Creation of Potentials\n",
    "\n",
    "A potential can be created via the `Potential` function, in a way very similar to `Deterministic`'s named interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with disaster_model:\n",
    "    \n",
    "    rate_constraint = pm.Potential('rate_constraint', pm.switch(pm.abs_(early_mean-late_mean)>1, -np.inf, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes just a `name` as its first argument and an expression returning the appropriate log-probability as the second argument.\n",
    "\n",
    "A common use of a factor potential is to represent an observed likelihood, where the observations are partly a function of model variables. In the contrived example below, we are representing the error in a linear regression model as a zero-mean normal random variable. Thus, the \"data\" in this scenario is the residual, which is a function both of the data and the regression parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied log-transform to sigma and added transformed sigma_log to model.\n"
     ]
    }
   ],
   "source": [
    "y = np.array([15, 10, 16, 11, 9, 11, 10, 18, 11])\n",
    "x = np.array([1, 2, 4, 5, 6, 8, 19, 18, 12])\n",
    "\n",
    "with pm.Model() as arma_model:\n",
    "\n",
    "    sigma = pm.HalfCauchy('sigma', 5)\n",
    "    beta = pm.Normal('beta', 0, sd=2)\n",
    "    mu = pm.Normal('mu', 0, sd=10)\n",
    "\n",
    "    err = y - (mu + beta*x)\n",
    "                  \n",
    "    like = pm.Potential('like', pm.Normal.dist(0, sd=sigma).logp(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameterization would not be compatible with an observed stochastic, because the `err` term would become fixed in the likelihood and not be allowed to change during sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Bioassay model\n",
    "\n",
    "Gelman et al. (2003) present an example of an acute toxicity test, commonly performed on animals to estimate the toxicity of various compounds.\n",
    "\n",
    "In this dataset `log_dose` includes 4 levels of dosage, on the log scale, each administered to 5 rats during the experiment. The response variable is death, the number of positive responses to the dosage.\n",
    "\n",
    "The number of deaths can be modeled as a binomial response, with the probability of death being a linear function of dose:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y_i &\\sim \\text{Bin}(n_i, p_i) \\\\\n",
    "\\text{logit}(p_i) &= a + b x_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "The common statistic of interest in such experiments is the LD50, the dosage at which the probability of death is 50%.\n",
    "\n",
    "Specify this model in PyMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Log dose in each group\n",
    "log_dose = [-.86, -.3, -.05, .73]\n",
    "\n",
    "# Sample size in each group\n",
    "n = 5\n",
    "\n",
    "# Outcomes\n",
    "deaths = [0, 1, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with MCMC\n",
    "\n",
    "PyMC provides three objects that fit models:\n",
    "\n",
    "- `MCMC`, which coordinates Markov chain Monte Carlo algorithms. The actual work of updating stochastic variables conditional on the rest of the model is done by `StepMethod` objects.\n",
    "\n",
    "- `MAP`, which computes maximum *a posteriori* estimates.\n",
    "\n",
    "- `NormApprox`, the joint distribution of all stochastic variables in a model is approximated as normal using local information at the maximum *a posteriori* estimate.\n",
    "\n",
    "All three objects are subclasses of `Model`, which is PyMC's base class\n",
    "for fitting methods. `MCMC` and `NormApprox`, both of which can produce\n",
    "samples from the posterior, are subclasses of `Sampler`, which is PyMC's\n",
    "base class for Monte Carlo fitting methods. `Sampler` provides a generic\n",
    "sampling loop method and database support for storing large sets of\n",
    "joint samples. These base classes implement some basic methods that are\n",
    "inherited by the three implemented fitting methods, so they are\n",
    "documented at the end of this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC\n",
    "\n",
    "The `MCMC` class implements PyMC's core business: producing Markov chain Monte Carlo samples for\n",
    "a model's variables. Its primary job is to create and coordinate a collection of 'step\n",
    "methods', each of which is responsible for updating one or more\n",
    "variables. \n",
    "\n",
    "`MCMC` provides the following useful methods:\n",
    "\n",
    "`sample(iter, burn, thin, tune_interval, tune_throughout, save_interval, ...)`\n",
    ":   Runs the MCMC algorithm and produces the traces. The `iter` argument\n",
    "controls the total number of MCMC iterations. No tallying will be\n",
    "done during the first `burn` iterations; these samples will be\n",
    "forgotten. After this burn-in period, tallying will be done each\n",
    "`thin` iterations. Tuning will be done each `tune_interval`\n",
    "iterations. If `tune_throughout=False`, no more tuning will be done\n",
    "after the burnin period. The model state will be saved every\n",
    "`save_interval` iterations, if given.\n",
    "\n",
    "`isample(iter, burn, thin, tune_interval, tune_throughout, save_interval, ...)`\n",
    ":   An interactive version of `sample`. The sampling loop may be paused\n",
    "at any time, returning control to the user.\n",
    "\n",
    "`use_step_method(method, *args, **kwargs)`:\n",
    ":   Creates an instance of step method class `method` to handle some\n",
    "stochastic variables. The extra arguments are passed to the `init`\n",
    "method of `method`. Assigning a step method to a variable manually\n",
    "will prevent the `MCMC` instance from automatically assigning one.\n",
    "However, you may handle a variable with multiple step methods.\n",
    "\n",
    "`stats()`:\n",
    ":   Generate summary statistics for all nodes in the model.\n",
    "\n",
    "The sampler's MCMC algorithms can be accessed via the `step_method_dict`\n",
    "attribute. `M.step_method_dict[x]` returns a list of the step methods\n",
    "`M` will use to handle the stochastic variable `x`.\n",
    "\n",
    "After sampling, the information tallied by `M` can be queried via\n",
    "`M.db.trace_names`. In addition to the values of variables, tuning\n",
    "information for adaptive step methods is generally tallied. These\n",
    "‘traces’ can be plotted to verify that tuning has in fact terminated. After sampling ends you can retrieve the trace as\n",
    "`M.trace[’var_name’]`.\n",
    "\n",
    "We can instantiate a MCMC sampler for the bioassay example as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = pm.MCMC(gelman_bioassay, db='sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling methods\n",
    "\n",
    "Though finding the MAP is a fast and easy way of obtaining estimates of the unknown model parameters, it is limited because there is no associated estimate of uncertainty produced with the MAP estimates. Instead, a simulation-based approach such as Markov chain Monte Carlo (MCMC) can be used to obtain a Markov chain of values that, given the satisfaction of certain conditions, are indistinguishable from samples from the posterior distribution. \n",
    "\n",
    "To conduct MCMC sampling to generate posterior samples in PyMC3, we specify a **step method** object that corresponds to a particular MCMC algorithm, such as Metropolis, Slice sampling, or the No-U-Turn Sampler (NUTS). PyMC3's `step_methods` submodule contains the following samplers: `NUTS`, `Metropolis`, `Slice`, `HamiltonianMC`, and `BinaryMetropolis`. These step methods can be assigned manually, or assigned automatically by PyMC3. Auto-assignment is based on the attributes of each variable in the model. In general:\n",
    "\n",
    "* Binary variables will be assigned to `BinaryMetropolis`\n",
    "* Discrete variables will be assigned to `Metropolis`\n",
    "* Continuous variables will be assigned to `NUTS`\n",
    "\n",
    "Auto-assignment can be overriden for any subset of variables by specifying them manually prior to sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step methods\n",
    "\n",
    "Step method objects handle individual stochastic variables, or sometimes groups \n",
    "of them. They are responsible for making the variables they handle take single \n",
    "MCMC steps conditional on the rest of the model. Each subclass of \n",
    "``StepMethod`` implements a method called ``step()``, which is called by \n",
    "``MCMC``. Step methods with adaptive tuning parameters can optionally implement \n",
    "a method called ``tune()``, which causes them to assess performance (based on \n",
    "the acceptance rates of proposed values for the variable) so far and adjust.\n",
    "\n",
    "The major subclasses of ``StepMethod`` are ``Metropolis`` and\n",
    "``AdaptiveMetropolis``. PyMC provides several flavors of the \n",
    "basic Metropolis steps.\n",
    "\n",
    "### Metropolis\n",
    "\n",
    "``Metropolis`` and subclasses implement Metropolis-Hastings steps. To tell an \n",
    "``MCMC`` object :math:`M` to handle a variable :math:`x` with a Metropolis step \n",
    "method, you might do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M.use_step_method(pm.Metropolis, M.alpha, proposal_sd=1., proposal_distribution='Normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Metropolis` itself handles float-valued variables, and subclasses\n",
    "`DiscreteMetropolis` and `BinaryMetropolis` handle integer- and\n",
    "boolean-valued variables, respectively.\n",
    "\n",
    "`Metropolis`' `__init__` method takes the following arguments:\n",
    "\n",
    "`stochastic`\n",
    ":   The variable to handle.\n",
    "\n",
    "`proposal_sd`\n",
    ":   A float or array of floats. This sets the proposal standard deviation if the proposal distribution is normal.\n",
    "\n",
    "`scale`\n",
    ":   A float, defaulting to 1. If the `scale` argument is provided but not `proposal_sd`, `proposal_sd` is computed as follows:\n",
    "\n",
    "```python\n",
    "if all(self.stochastic.value != 0.):\n",
    "    self.proposal_sd = (ones(shape(self.stochastic.value)) * \n",
    "                   abs(self.stochastic.value) * scale)\n",
    "else:\n",
    "    self.proposal_sd = ones(shape(self.stochastic.value)) * scale\n",
    "```\n",
    "\n",
    "`proposal_distribution`\n",
    ":   A string indicating which distribution should be used for proposals.\n",
    "Current options are `'Normal'` and `'Prior'`. If\n",
    "`proposal_distribution=None`, the proposal distribution is chosen\n",
    "automatically. It is set to `'Prior'` if the variable has no\n",
    "children and has a random method, and to `'Normal'` otherwise.\n",
    "\n",
    "Alhough the `proposal_sd` attribute is fixed at creation, Metropolis\n",
    "step methods adjust their initial proposal standard deviations using an\n",
    "attribute called `adaptive_scale_factor`. During tuning, the\n",
    "acceptance ratio of the step method is examined, and this scale factor\n",
    "is updated accordingly. If the proposal distribution is normal,\n",
    "proposals will have standard deviation\n",
    "`self.proposal_sd * self.adaptive_scale_factor`.\n",
    "\n",
    "By default, tuning will continue throughout the sampling loop, even\n",
    "after the burnin period is over. This can be changed via the\n",
    "`tune_throughout` argument to `MCMC.sample`. If an adaptive step\n",
    "method's `tally` flag is set (the default for `Metropolis`), a trace of\n",
    "its tuning parameters will be kept. If you allow tuning to continue\n",
    "throughout the sampling loop, it is important to verify that the\n",
    "'Diminishing Tuning' condition of [Roberts and Rosenthal (2007)](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.jap/1183667414) is satisfied: the\n",
    "amount of tuning should decrease to zero, or tuning should become very\n",
    "infrequent.\n",
    "\n",
    "If a Metropolis step method handles an array-valued variable, it\n",
    "proposes all elements independently but simultaneously. That is, it\n",
    "decides whether to accept or reject all elements together but it does\n",
    "not attempt to take the posterior correlation between elements into\n",
    "account. The `AdaptiveMetropolis` class (see below), on the other hand,\n",
    "does make correlated proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiscreteMetropolis\n",
    "\n",
    "This class is just like `Metropolis`, but specialized to handle\n",
    "`Stochastic` instances with dtype `int`. The jump proposal distribution\n",
    "can either be `'Normal'`, `'Prior'` or `'Poisson'` (the default). In the\n",
    "normal case, the proposed value is drawn from a normal distribution\n",
    "centered at the current value and then rounded to the nearest integer.\n",
    "\n",
    "### BinaryMetropolis\n",
    "\n",
    "This class is specialized to handle `Stochastic` instances with dtype\n",
    "`bool`.\n",
    "\n",
    "For array-valued variables, `BinaryMetropolis` can be set to propose\n",
    "from the prior by passing in `dist=\"Prior\"`. Otherwise, the argument\n",
    "`p_jump` of the init method specifies how probable a change is. Like\n",
    "`Metropolis`' attribute `proposal_sd`, `p_jump` is tuned throughout the\n",
    "sampling loop via `adaptive_scale_factor`.\n",
    "\n",
    "### Automatic assignment of step methods\n",
    "\n",
    "Every step method subclass (including user-defined ones) that does not\n",
    "require any `__init__` arguments other than the stochastic variable to\n",
    "be handled adds itself to a list called `StepMethodRegistry` in the PyMC\n",
    "namespace. If a stochastic variable in an `MCMC` object has not been\n",
    "explicitly assigned a step method, each class in `StepMethodRegistry` is\n",
    "allowed to examine the variable.\n",
    "\n",
    "To do so, each step method implements a class method called\n",
    "`competence(stochastic)`, whose only argument is a single stochastic\n",
    "variable. These methods return values from 0 to 3; 0 meaning the step\n",
    "method cannot safely handle the variable and 3 meaning it will most\n",
    "likely perform well for variables like this. The `MCMC` object assigns\n",
    "the step method that returns the highest competence value to each of its\n",
    "stochastic variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running MCMC Samplers\n",
    "\n",
    "We can carry out Markov chain Monte Carlo sampling by calling the `sample` method (or in the terminal, `isample`) with the appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = pm.MCMC(gelman_bioassay)\n",
    "M.sample(10000, burn=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pm.Matplot.plot(M.LD50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of Missing Data\n",
    "\n",
    "As with most textbook examples, the models we have examined so far\n",
    "assume that the associated data are complete. That is, there are no\n",
    "missing values corresponding to any observations in the dataset.\n",
    "However, many real-world datasets have missing observations, usually due\n",
    "to some logistical problem during the data collection process. The\n",
    "easiest way of dealing with observations that contain missing values is\n",
    "simply to exclude them from the analysis. However, this results in loss\n",
    "of information if an excluded observation contains valid values for\n",
    "other quantities, and can bias results. An alternative is to impute the\n",
    "missing values, based on information in the rest of the model.\n",
    "\n",
    "For example, consider a survey dataset for some wildlife species:\n",
    "\n",
    "    Count   Site   Observer   Temperature\n",
    "    ------- ------ ---------- -------------\n",
    "    15      1      1          15\n",
    "    10      1      2          NA\n",
    "    6       1      1          11\n",
    "\n",
    "Each row contains the number of individuals seen during the survey,\n",
    "along with three covariates: the site on which the survey was conducted,\n",
    "the observer that collected the data, and the temperature during the\n",
    "survey. If we are interested in modelling, say, population size as a\n",
    "function of the count and the associated covariates, it is difficult to\n",
    "accommodate the second observation because the temperature is missing\n",
    "(perhaps the thermometer was broken that day). Ignoring this observation\n",
    "will allow us to fit the model, but it wastes information that is\n",
    "contained in the other covariates.\n",
    "\n",
    "In a Bayesian modelling framework, missing data are accommodated simply\n",
    "by treating them as unknown model parameters. Values for the missing\n",
    "data $\\tilde{y}$ are estimated naturally, using the posterior predictive\n",
    "distribution:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "This describes additional data $\\tilde{y}$, which may either be\n",
    "considered unobserved data or potential future observations. We can use\n",
    "the posterior predictive distribution to model the likely values of\n",
    "missing data.\n",
    "\n",
    "Consider the coal mining disasters data introduced previously. Assume\n",
    "that two years of data are missing from the time series; we indicate\n",
    "this in the data array by the use of an arbitrary placeholder value,\n",
    "`None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disasters_missing = np.array([ 4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n",
    "3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n",
    "2, 2, 3, 4, 2, 1, 3, -999, 2, 1, 1, 1, 1, 3, 0, 0,\n",
    "1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n",
    "0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n",
    "3, 3, 1, -999, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n",
    "0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate these values in PyMC, we generate a *masked array*. These are specialised NumPy arrays that contain a matching True or False value for each element to indicate if that value should be excluded from any computation. Masked arrays can be generated using NumPy's `ma.masked_equal` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_array(data = [4 5 4 0 1 4 3 4 0 6 3 3 4 0 2 6 3 3 5 4 5 3 1 4 4 1 5 5 3 4 2 5 2 2 3 4 2\n",
       " 1 3 -- 2 1 1 1 1 3 0 0 1 0 1 1 1 0 1 0 1 0 0 0 2 1 0 0 0 1 1 0 2 3 3 1 --\n",
       " 2 1 1 1 1 2 4 2 0 0 1 4 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1],\n",
       "             mask = [False False False False False False False False False False False False\n",
       " False False False False False False False False False False False False\n",
       " False False False False False False False False False False False False\n",
       " False False False  True False False False False False False False False\n",
       " False False False False False False False False False False False False\n",
       " False False False False False False False False False False False False\n",
       " False False False False False False False False False False False  True\n",
       " False False False False False False False False False False False False\n",
       " False False False False False False False False False False False False\n",
       " False False False],\n",
       "       fill_value = -999)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disasters_masked = np.ma.masked_values(disasters_missing, value=-999)\n",
    "disasters_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This masked array, in turn, can then be passed to one of PyMC's data stochastic variables, which recognizes the masked array and replaces the missing values with stochastic variables of the desired type. For the coal mining disasters problem, recall that disaster events were modeled as Poisson variates:\n",
    "\n",
    "```python\n",
    "disasters = Poisson('disasters', mu=rate, observed=masked_values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in `disasters` is a Poisson random variable, irrespective of whether the observation was missing or not. The difference is that actual observations are assumed to be data stochastics, while the missing\n",
    "values are unobserved stochastics. The latter are considered unknown, rather than fixed, and therefore estimated by the fitting algorithm, just as unknown model parameters are.\n",
    "\n",
    "The entire model looks very similar to the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied log-transform to early_mean and added transformed early_mean_log to model.\n",
      "Applied log-transform to late_mean and added transformed late_mean_log to model.\n"
     ]
    }
   ],
   "source": [
    "with Model() as missing_data_model:\n",
    "\n",
    "    # Prior for distribution of switchpoint location\n",
    "    switchpoint = DiscreteUniform('switchpoint', lower=0, upper=len(disasters_masked))\n",
    "    # Priors for pre- and post-switch mean number of disasters\n",
    "    early_mean = Exponential('early_mean', lam=1.)\n",
    "    late_mean = Exponential('late_mean', lam=1.)\n",
    "\n",
    "    # Allocate appropriate Poisson rates to years before and after current\n",
    "    # switchpoint location\n",
    "    idx = np.arange(len(disasters_masked))\n",
    "    rate = Deterministic('rate', switch(switchpoint >= idx, early_mean, late_mean))\n",
    "\n",
    "    # Data likelihood\n",
    "    disasters = Poisson('disasters', rate, observed=disasters_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have used the `masked_array` function, rather than\n",
    "`masked_equal`, and the value -999 as a placeholder for missing data.\n",
    "The result is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Metropolis to switchpoint\n",
      "Assigned NUTS to early_mean_log\n",
      "Assigned NUTS to late_mean_log\n",
      "Assigned Metropolis to disasters_missing\n",
      " [-----------------100%-----------------] 2000 of 2000 complete in 2.5 sec"
     ]
    }
   ],
   "source": [
    "with missing_data_model:\n",
    "    trace_missing = sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[switchpoint, early_mean_log, late_mean_log, disasters_missing]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data_model.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import forestplot\n",
    "\n",
    "forestplot(trace_missing, varnames=['disasters_missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. M.I. Jordan. Graphical models. Statist. Sci., 19(1):140–155, 2004."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
